{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee9d140-0c81-4867-99aa-b90ef83b02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b6dffe-0df8-4b4f-823e-3b2d79c452d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame extraction function\n",
    "def extract_frames(video_path, output_folder, frame_rate=1):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    success = True\n",
    "\n",
    "    while success:\n",
    "        success, frame = video_capture.read()\n",
    "        if success and frame_count % frame_rate == 0:\n",
    "            frame_filename = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n",
    "            cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c3a16a-45b2-4619-b33b-b6972fbe9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def extract_skeleton_keypoints(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:\n",
    "        results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.append((landmark.x, landmark.y))\n",
    "            return image_path, np.array(keypoints).flatten()\n",
    "        else:\n",
    "            return image_path, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d72c5ed-919c-499e-9a0c-12fda191a969",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(csv_filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43msave_keypoints_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeypoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeypoints_lstm.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36msave_keypoints_to_csv\u001b[1;34m(images_folder, csv_filename)\u001b[0m\n\u001b[0;32m     10\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend([image_path] \u001b[38;5;241m+\u001b[39m keypoints\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a DataFrame and save to CSV\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeypoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(csv_filename, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def save_keypoints_to_csv(images_folder, csv_filename):\n",
    "    data = []\n",
    "    for image_filename in os.listdir(images_folder):\n",
    "        image_path = os.path.join(images_folder, image_filename)\n",
    "        image_path, keypoints = extract_skeleton_keypoints(image_path)\n",
    "        if keypoints is not None:\n",
    "            data.append([image_path] + keypoints.tolist())\n",
    "    \n",
    "    # Create a DataFrame and save to CSV\n",
    "    columns = ['image_path'] + [f'keypoint_{i}' for i in range(len(data[0]) - 1)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "# Example usage\n",
    "save_keypoints_to_csv('keypoints', 'keypoints_lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5a298b-0b1d-4a54-96ac-641201bd4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preparation function\n",
    "def prepare_dataset(frames_folder, label, sequence_length=10):\n",
    "    data = []\n",
    "    sequence = []\n",
    "    for frame_filename in sorted(os.listdir(frames_folder)):\n",
    "        frame_path = os.path.join(frames_folder, frame_filename)\n",
    "        keypoints = extract_skeleton_keypoints(frame_path)\n",
    "        if keypoints is not None:\n",
    "            sequence.append(keypoints)\n",
    "            if len(sequence) == sequence_length:\n",
    "                data.append((np.array(sequence), label))\n",
    "                sequence = []\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b393b7-ee30-4195-80c3-837f878778e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton keypoints extraction function\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "def extract_skeleton_keypoints(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:\n",
    "        results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if results.pose_landmarks:\n",
    "            keypoints = []\n",
    "            for landmark in results.pose_landmarks.landmark:\n",
    "                keypoints.append((landmark.x, landmark.y))\n",
    "            return np.array(keypoints).flatten()\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d784b6-7eba-4d24-8520-35b806e29095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM-based pose classifier model\n",
    "class PoseLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(PoseLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(2, x.size(0), hidden_dim).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return torch.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d1f40a5-7b49-4ed8-968c-6239e2d44f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR MULTIPLE CLASSES\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6bd9e7-9cc1-4c8f-9f85-7b65f979266d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Convert to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     32\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 33\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#1\u001b[39;00m\n\u001b[0;32m     34\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(features, labels)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Split into training and testing datasets\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Path to the folder containing the videos\n",
    "    #folder = 'D:/anantasana'\n",
    "    #class_folders = ['Anantasana Right Steps', 'Ardhakati Chakrasana Right Steps']  # Update with actual folder names\n",
    "    frame_storage_folders = ['class1_frames', 'class2_frames']\n",
    "\n",
    "    # Create directories to store frames for each class\n",
    "    #for frame_storage_folder in frame_storage_folders:\n",
    "    #    os.makedirs(frame_storage_folder, exist_ok=True)\n",
    "\n",
    "    # Extract frames from videos in each class\n",
    "   # for class_folder, frame_storage_folder in zip(class_folders, frame_storage_folders):\n",
    "   #     class_folder_path = os.path.join(folder, class_folder)\n",
    "   #     for video_filename in os.listdir(class_folder_path):\n",
    "        #    video_path = os.path.join(class_folder_path, video_filename)\n",
    "         #   extract_frames(video_path, frame_storage_folder)\n",
    "\n",
    "    # Prepare datasets for each class\n",
    "    datasets = []\n",
    "    for i, frame_storage_folder in enumerate(frame_storage_folders):\n",
    "        class_data = prepare_dataset(frame_storage_folder, i)\n",
    "        datasets.extend(class_data)\n",
    "\n",
    "    # Combine and shuffle the dataset\n",
    "    np.random.shuffle(datasets)\n",
    "\n",
    "    # Split features and labels\n",
    "    X = np.array([item[0] for item in datasets])\n",
    "    y = np.array([item[1] for item in datasets])\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    features = torch.tensor(X, dtype=torch.float32)\n",
    "    labels = torch.tensor(y, dtype=torch.float32).unsqueeze(1) #1\n",
    "    dataset = TensorDataset(features, labels)\n",
    "\n",
    "    # Split into training and testing datasets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    input_dim = 66  # Assuming 33 keypoints with x and y coordinates\n",
    "    hidden_dim = 128\n",
    "    output_dim = 2\n",
    "\n",
    "    model = PoseLSTMClassifier(input_dim, hidden_dim, output_dim)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "            train_total += batch_labels.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_labels in test_loader:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                test_correct += (predicted == batch_labels).sum().item()\n",
    "                test_total += batch_labels.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = test_correct / test_total\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.6f}, Train Accuracy: {train_accuracy:.2f}, Test Loss: {test_loss:.6f}, Test Accuracy: {test_accuracy:.2f}')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'pose_lstm_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0768d-6bc9-423e-8727-697e7ccdc46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fd05a4-bc1a-4bd5-870d-14c9b3466562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
